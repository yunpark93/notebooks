{"cells":[{"cell_type":"markdown","collapsed":false,"source":["# Using scikit-learn calibration"],"language":"python","metadata":{},"outputs":[],"prompt_number":0},{"cell_type":"code","collapsed":false,"source":["import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier \nfrom sklearn.metrics import log_loss\nfrom sklearn.calibration import CalibratedClassifierCV\n\nprint \"hello\""],"language":"python","metadata":{},"outputs":[],"prompt_number":1},{"cell_type":"markdown","collapsed":false,"source":["# Import Data (Kaggle OTTO challenge)"],"language":"python","metadata":{},"outputs":[],"prompt_number":2},{"cell_type":"code","collapsed":false,"source":["X = pd.read_csv('../train.csv')\nX = X.drop('id', axis=1)\n\n# Extract target\n# Encode it to make it manageable by ML algo\ny = X.target.values\ny = LabelEncoder().fit_transform(y)\n\n# Remove target from train, else it's too easy ...\nX = X.drop('target', axis=1)\n\nX.head(5)"],"language":"python","metadata":{},"outputs":[],"prompt_number":3},{"cell_type":"markdown","collapsed":false,"source":["# Split Train / Test"],"language":"python","metadata":{},"outputs":[],"prompt_number":4},{"cell_type":"code","collapsed":false,"source":["Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.20, random_state=36)"],"language":"python","metadata":{},"outputs":[],"prompt_number":5},{"cell_type":"markdown","collapsed":false,"source":["# Train and apply a Random Forest (without calibration)"],"language":"python","metadata":{},"outputs":[],"prompt_number":6},{"cell_type":"code","collapsed":false,"source":["clf = RandomForestClassifier(n_estimators=250, n_jobs=-1)\n# we use a BaggingClassifier to make 5 predictions, and average\n# beacause that's what CalibratedClassifierCV do behind the scene\n# and we want to compare things fairly\nclfbag = BaggingClassifier(clf, n_estimators=5)\nclfbag.fit(Xtrain, ytrain)\nypreds = clfbag.predict_proba(Xtest)\nprint \"%.2f\" % log_loss(ytest, ypreds, eps=1e-15, normalize=True)"],"language":"python","metadata":{},"outputs":[],"prompt_number":7},{"cell_type":"markdown","collapsed":false,"source":["# Train and apply a Random Forest (with calibration)"],"language":"python","metadata":{},"outputs":[],"prompt_number":8},{"cell_type":"code","collapsed":false,"source":["clf = RandomForestClassifier(n_estimators=250, n_jobs=-1)\n# in our case, 'isotonic' works better than default 'sigmoid'\ncalibrated_clf = CalibratedClassifierCV(clf, method='isotonic', cv=5)\ncalibrated_clf.fit(Xtrain, ytrain)\nypreds = calibrated_clf.predict_proba(Xtest)\nprint \"%.2f\" % log_loss(ytest, ypreds, eps=1e-15, normalize=True)"],"language":"python","metadata":{},"outputs":[],"prompt_number":9},{"cell_type":"markdown","collapsed":false,"source":["# We highly improved performance with calibration !"],"language":"python","metadata":{},"outputs":[],"prompt_number":10}],"metadata":{"name":"export","notebookId":1715147},"nbformat":4,"nbformat_minor":0}